<!DOCTYPE HTML>
<html lang="en">

	<head>
		<script>
			var nextPage = "/statistics/gaussian-distribution/";
			var prevPage = "/statistics/weak-law-of-large-numbers/";
			var nextPageTitle = "Gaussian Distribution";
			var prevPageTitle = "Law of Large Numbers";
			var arrowLeftPage = "/statistics/central-limit-theorem/";
			var arrowRightPage = "/statistics/central-limit-theorem/julia/";
		</script>

		<link rel="stylesheet" href="/data/css/main.css">
		
		<title>Python | Central Limit Theorem | Fundamentals of Statistics - QuantML</title>
		<meta charset="utf-8" />
		<link rel="icon" href="/data/icon.png" type="image/png" sizes="16x16">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="robots" content="index, follow">

		<script>importPrism = true</script>
		<script src="/data/js/initialize.js"></script>

	</head>

	<body>

		<!-- Wrapper -->
			<div style="display: none;" id="wrapper">
				<div class="bg fixed" style="transform: none;"></div>
				<div id="navPanelToggle">Menu</div>


				<!-- Header -->
				<header id="header">
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-light"><img src="/data/img/cover.webp" alt="QuantML.org" /></a>
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-dark"><img src="/data/img-dark/cover.webp" alt="QuantML.org" /></a>
					<script>if(window.quantml["theme"]=="dark") document.getElementById('quantml-cover-dark').style.display = "block";else document.getElementById('quantml-cover-light').style.display = "block";</script>
				</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="cover"><a href="/statistics/"><img src="/data/img/STATISTICS-cover.png" alt="STATISTICS"></a></li>
							<li class="link"><a href="/statistics/central-limit-theorem/">Central Limit Theorem</a></li>
							<li class="active python"><img src="/data/img/python.png" alt="Python"></li>
							<li class="julia"><a href="/statistics/central-limit-theorem/julia/"><img src="/data/img/julia.png" alt="Julia"></a></li>
						</ul>
					</nav>

<!-- Main -->
<div id="main">
    <section class="post">
<div style="text-align: center;" id="load-init"></div>
<script>initializeBody();</script>

<div id="paragraph-content" data-swipe-threshold="80">

<div id="desktop-mode">
	<div id="btn-container"></div><br>
</div>

<blockquote class="sidebar">
	First let's see how we can perform a <b>single simulation</b>, then we will see how to perform <b>multiple simulations</b> and <b>visualize</b> the Weak Law of Large Numbers.
</blockquote><br>

<h1 id="single-simulation">Single Simulation <img src="/data/img/python.png" alt="Python" width="112.5px" height="30px"><a href="#single-simulation" aria-label="single-simulation"><i class="fa fa-link" aria-hidden="true"></i></a></h1>

Assume that there are \(1000\) students in your school and you want to know, what is the average height of \(100\) randomly selected students? And how is that average height distributed (or say the distribution of that average height)?<br>

<a href="/statistics/introduction/#dogma">Remember that dogma</a> <br>
<img id='dogma' class="full-size-img" src="/statistics/img/dogma.png" alt="Central Dogma of Probability and Statistics">

<h2 id="truth">Truth<a href="#truth" aria-label="truth"><i class="fa fa-link" aria-hidden="true"></i></a></h2>
Now let's define the <b>truth</b>. <br>
Say that every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a> with range of \([165\text{ cm },185\text{ cm}]\). <br>
So \(X_1,\cdots,X_{50}\sim\text{Unif}[165, 185]\) <br>
<blockquote class="noborder">
	<b>Note:</b> This information is completely unknown to us.
</blockquote>
<br>

<h2 id="probability">Probability<a href="#probability" aria-label="probability"><i class="fa fa-link" aria-hidden="true"></i></a></h2>
We use probability to generate data using the <b>Truth</b> we defined above.<br>
Now let's create the population of all \(1000\) students. <br>

<pre data-start="1"><code class="language-python line-numbers">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform # to draw random variables from Uniform distribution

N = 1000        # population size
a = 165
scale = 20

# Unif[a, a + scale]
distribution = uniform(a, scale)
population = distribution.rvs(N)
</code></pre>
<br>

<h2 id="observation">Observation<a href="#observation" aria-label="observation"><i class="fa fa-link" aria-hidden="true"></i></a></h2>
Now that we have all \(1000\) students, and every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a>. <br>
Let's (randomly) take a sample of \(100\) students
<pre data-start="11"><code class="language-python line-numbers">n = 100
sample = np.random.choice(population,n)
</code></pre>
<br>

<h2 id="statistics">Statistics<a href="#statistics" aria-label="statistics"><i class="fa fa-link" aria-hidden="true"></i></a></h2>
So now we have our sample of \(100\) students, let's estimate the average height of those \(100\) students <b>(sample mean)</b>. <br>

<pre data-start="13"><code class="language-python line-numbers">sample_mean = np.mean(sample)
</code></pre>

<span class="bb">sample_mean</span> is the average height of \(100\) randomly selected students. <br>

Now let's see how the distribution of the average height of \(100\) randomly selected students look like. <br>
To achieve this we need to perform this simulation multiple times and plot all sample means as a histogram. <br>
<br>

<h1 id="multiple-simulations">Multiple simulations <img src="/data/img/python.png" alt="Python" width="112.5px" height="30px"><a href="#multiple-simulations" aria-label="multiple-simulations"><i class="fa fa-link" aria-hidden="true"></i></a></h1>
The <b>truth</b> is that every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a> \(\text{Unif}[165, 185]\), so let's start with creating population. <br>

<pre data-start="1"><code class="language-python line-numbers">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform # to draw random variables from Uniform distribution
np.random.seed(1)

N = 1000        # population size
a = 165
scale = 20

# Unif[a, a + scale]
distribution = uniform(a, scale)
population = distribution.rvs(N)
</code></pre>
<br>

Now let's collect <span class="bb">n_simulations</span> sample means of sample size \((n) = 30\). <br>
(say n_simulations = 200)
<pre data-start="12"><code class="language-python line-numbers">n = 30
n_simulations = 200
sample_means = np.zeros(n_simulations)
for k in range(n_simulations):
	sample_means[k] = np.mean(np.random.choice(population,n))
</code></pre>
Now let's plot these sample means. <br>

<pre data-start="17"><code class="language-python line-numbers">num_bins = 50
n, bins, patches = plt.hist(sample_means, num_bins, facecolor='blue', alpha=0.5)
plt.xlabel("Sample mean")
plt.ylabel("# occurrences of sample mean")
plt.show()
</code></pre>

<img class="full-size-img" src="/statistics/img/central-limit-theorem/clt-sampeling-distribution.png" alt="">

<div style="text-align: center;">Sample mean seems to be normally distributed.</div>
Well it's a Central Limit Theorem Simulation right, and the whole point of CLT is to approximate probability distribution of sample mean \(\overline{X}_n\) to it's corresponding Normal distribution, <b>am I correct?</b> <br>
Let's see ourselves. Let's overlay a Normal distribution with true mean and variance of <span class="bb">sample_means</span>. <br>

<pre data-start="17"><code class="language-python line-numbers">from scipy.stats import norm
samples_mean = np.mean(sample_means)
samples_std = np.std(sample_means)

fig, ax = plt.subplots()

# Getting PDF
x = np.linspace(samples_mean - 3*samples_std, samples_mean + 3*samples_std, 100)
pdf = norm.pdf(x, samples_mean, samples_std)
ax.plot(x, pdf, c="g", label="Normal distribution (PDF)")

# Getting Histogram
num_bins = 50
counts, bins = np.histogram(sample_means,
							np.linspace(min(sample_means),
										max(sample_means),
										num_bins)
							)
n, bins, patches = ax.hist(sample_means,
							bins[:-1],
							facecolor='blue',
							alpha=0.5,
							color="b",
							weights=(1/sum(counts))*np.ones_like(sample_means),
							label="Sampling distribution")
leg = ax.legend();
</code></pre>
<img class="full-size-img" src="/statistics/img/central-limit-theorem/clt-sim-pdf.png" alt="">
Here height of the bins of Sampling distribution represent the probability of falling into that bin. <br>
The Normal distribution that we plotted have same <b>mean</b> and <b>variance</b> as of our Sampling distribution, but their shapes are way off. <br>

<br>
<details>
	<summary><i><span class="bb">Sampling distribution of \(\sqrt{n}\, (\overline{X}_n-\mu)\) is much better fit for Normal distribution</span></i></summary>
<blockquote class="noborder">
	If you follow the result of the CLT,
	\[\sqrt{n}\, (\overline{X}_n-\mu) \xrightarrow [n\to \infty ]{(d)} \mathcal{N}(0,\sigma^2) \]
	Then you can see our <b>normally distributed</b> data. <br>
	We just have to center our observations around \(0\) and then stretch it by a factor of \(\sqrt{n}\). <br>

	<pre data-start="17"><code class="language-python line-numbers">from scipy.stats import norm
mean = np.mean(np.sqrt(n) * (sample_means - distribution.mean()))
std = np.sqrt(n) * np.std(sample_means)

# Centered and stretched sample mean (r.v.)
sample_means = np.sqrt(n) * (sample_means - distribution.mean())

fig, ax = plt.subplots()

# Getting PDF N(0, Ïƒ)
x = np.linspace(- 3*std, 3*std, 100)
pdf = norm.pdf(x, 0, std)
ax.plot(x, pdf, c="g", label="Normal distribution (PDF)")

# Getting Histogram
num_bins = 50
counts, bins = np.histogram(sample_means,
							np.linspace(min(sample_means),
										max(sample_means),
										num_bins)
							)
n, bins, patches = ax.hist(sample_means,
							bins[:-1],
							facecolor='blue',
							alpha=0.5,
							color="b",
							weights=(1/sum(counts))*np.ones_like(sample_means),
							label="Sampling distribution")
leg = ax.legend();
</code></pre>
<img class="full-size-img" src="/statistics/img/central-limit-theorem/clt-centered.png" alt="">
Now the plot feels much more <b>Normal</b> but here the random variable is \(\sqrt{n}\, (\overline{X}_n-\mu) \) not \(\overline{X}_n\). <br>

<br>
</blockquote>
</details>
<br>

And as we <a href="/statistics/central-limit-theorem/#its-about-CDF">discussed earlier</a> that,
<blockquote class="sidebar">
	<b>Central Limit Theorem</b> is <b>not</b> a statement about the convergence of PDF or PMF. It's a statement about the convergence of <b>CDF</b>.
</blockquote>

Now let's see ourselves,

<pre data-start="44"><code class="language-python line-numbers"># Plot CDF
fig, ax = plt.subplots()
# CDF of Normal Distribution
cdf = norm.cdf(x, samples_mean, samples_std)
ax.plot(x, cdf, c='g', label="Normal distribution CDF")

# CDF of Sampling Distribution
cdf = np.cumsum(n)/sum(n)
ax.scatter(bins[:-1], cdf, c='b', marker="*", label="Sampling distribution CDF")
leg = ax.legend();
plt.show()
</code></pre>

<img class="full-size-img" src="/statistics/img/central-limit-theorem/clt-sim-cdf.png" alt="">
Here we can see the beauty of <b>Central limit theorem</b>, we can see how close the CDF of our Sampling distribution get closer to the CDF of a Normal distribution. <br>

Now try tweaking <span class="bb">\(n\)</span> and see how it affects the CDF convergence. <br>
Also try different distributions form <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy.stats</a> <br>

<blockquote class="noborder align-center">
	<a href="/statistics/central-limit-theorem/julia/"><img src="/data/img/julia.png" alt="Julia" width="53px" height="33px"> Simulation</a>
	<br>
    <a rel="noreferrer" target="_blank" href="https://app.quantml.org/statistics/?ch=Central-Limit-Theorem&dist=Uniform+distribution">Visualize Central Limit Theorem &nbsp;<img src="/data/img/app.png" alt="launch" width="30px" height="30px"></a> <br>
    Try different distributions, tweak there parameters, sample size, number of simulations and see how it impacts the convergence of <b>CDF</b> of \(\overline{X}_n\).
</blockquote><br>




</div>

<div id="btn-container"></div><br>


<div id="modals-html"></div>

</section>
</div><!-- Main [END]-->
</div><!-- Wrapper [END]-->
<script>requireScript('script-js', '0.1.0', '/data/js/script.js', function(){cssLoaded()})</script>

</body>
</html>