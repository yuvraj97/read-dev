<!DOCTYPE HTML>
<!--
    "Thanks to HTML5 UP for this template".
    Massively by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>

        <!--Prism-->
        <link rel="stylesheet" href="/data/prism/prism.css">
    


        <link id="quantml-theme" rel="stylesheet" href="" />
        <link id="quantml-auth-style" rel="stylesheet" href="">
        <script src="/data/initialize-css.js"></script>

        <!--KATEX-->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

        <!-- The loading of KaTeX is deferred to speed up page rendering -->
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>

        <!-- To automatically render math in text elements, include the auto-render extension: -->
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        
		<title>Diagonalization of a Matrix</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<noscript><link rel="stylesheet" href="/data/assets/css/noscript.css" /></noscript>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        
        <link rel="icon" href="/data/icon.png" type="image/png" sizes="16x16">

	</head>   
    
    <body style="display: none;">

        <!-- Wrapper -->
            <div id="wrapper">

                <!-- Header -->
                    <header id="header">
                        <a target="_blank" href="https://quantml.org" class="logo image jump-big"><img src="/data/img/cover.png" alt="QuantML.org" /></a>
                    </header>

                <!-- Nav -->
                    <nav id="nav">
                        <ul class="links">
                            <li class="cover"><a href="/la/"><img src="/data/img/linear-algebra-cover.png" alt="Linear Algebra"></a></li>
                            <li class="active"><a href="/la/diagonalization/">Diagonalization of a Matrix</a></li>
                        </ul>
                        <ul class="icons">
                            <li><a target="_blank" href="https://quantml.org/"><img src="/data/img/icon.png" alt="QuantML"></a></li>
                            <li><a target="_blank" href="https://join.slack.com/t/quantml-org/shared_invite/zt-hcmbg7fr-jPbVAUT_tjGPaKWU50qMYQ"><img src="/data/img/slack.png" alt="Slack"></a></li>
                            <li><a target="_blank" href="https://www.linkedin.com/in/yuvraj97/"><img src="/data/img/linkedin.png" alt="LinkedIn"></a></li>
                            <li><a target="_blank" href="https://github.com/yuvraj97/"><img src="/data/img/github.png" alt="GitHub"></a></li>
                            <li><a style="cursor: pointer;"><img id="change-theme" src="" alt="Change Theme"></a></li>
                        </ul>
                    </nav>
                <!-- Main -->
                    <div id="main">
                        <section class="post">
<div id="btn-container">
    <button id="prev-btn" class="button" onclick="window.location.href = '/la/eigenvalues-eigenvectors/';">&#x25C0;&nbsp;&nbsp;Eigenvalues and Eigenvectors</button>
    <button id="next-btn" class="button"  style="float: right;" onclick="window.location.href = '/la/differential-equation/';">Differential Equation&nbsp;&nbsp;&#x25B6;</button>
</div><br>

<p>
<h2>Diagonalization of a Matrix</h2>
Say that we have a \(n\times n\) matrix \(A\) with 
\(n\) <b>independent</b> <i>eigenvectors</i> (say \(\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_n\)).<br>
We put these <i>eigenvectors</i> (\(\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_n\)) in a matrix (say \(S\)).<br>
\[
S=\begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 \vec{x}_{1} & \vec{x}_{2} & \cdots &  \vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix}
\]
\[A\vec{x}_i=\lambda_i\vec{x}_i;\quad 
\left\{\begin{matrix}
    \vec{x}_i\text{ is eigenvector}\\
    \lambda_i\text{ is eigenvalue}
    \end{matrix}\right.
\]
Let's calculate \(AS\)<br>
<blockquote class="noborder">
\(AS = A \begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 \vec{x}_{1} & \vec{x}_{2} & \cdots &  \vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix}\)<br>

\(\Rightarrow AS = \begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 A\vec{x}_{1} & A\vec{x}_{2} & \cdots &  A\vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix}\)<br>

\(\Rightarrow AS = \begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 \lambda_1\vec{x}_{1} & \lambda_2\vec{x}_{2} & \cdots &  \lambda_n\vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix}\)<br>

\(\Rightarrow AS = \begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 \vec{x}_{1} & \vec{x}_{2} & \cdots & \vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix}
 \underbrace{\begin{bmatrix}
 \lambda_1 &     0     & \cdots  &    0    \\  
 0         & \lambda_2 & \cdots  &    0    \\  
 \vdots    & \vdots    & \ddots  & \vdots    \\  
 0         &    0      & \cdots  &    \lambda_n    \\  
\end{bmatrix}}_{\text{say }\Lambda}
 \)<br>
 </blockquote>

<span class="bb">\(AS=S\Lambda\)</span><br>
because \(S\) has independent columns so \(S^{-1}\) exists, so<br>

<div class="math-border">
    \[
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        \displaystyle \Lambda =S^{-1}AS
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
    \]
</div>
    
This is <b>Diagonalization</b>.<br>

We can also get a factorization of \(A\)<br>
<div class="math-border">
    \[
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        \displaystyle A=S\Lambda S^{-1}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
    \]
</div><br>
<br>

<h2 id="power">Power of a matrix</h2>
Now let's see how \(A=S\Lambda S^{-1}\) helps us in calculating \(A^k\) for \(k=\{1,2,3,\cdots\}\)<br>
We know that,<br>
\[A\vec{x}_i=\lambda_i\vec{x}_i;\quad 
\left\{\begin{matrix}
    \vec{x}_i\text{ is eigenvector}\\
    \lambda_i\text{ is eigenvalue}
    \end{matrix}\right.
\]
Now let's calculate <i>eigenvectors</i> and <i>eigenvalues</i> of \(A^2\).<br>
\(AA\vec{x}_i=\lambda_i (A \vec{x}_i)\)<br>
\(\Rightarrow A^2\vec{x}_i=\lambda_i^2\vec{x}_i\)<br>
So <i>eigenvectors</i> of \(A^2\) remains same.<br>
But <i>eigenvalues</i> of \(A^2\) becomes \(\lambda^2\).<br>
<br>
We can also get it by \(A=S\Lambda S^{-1}\).<br>
<blockquote class="noborder">
\(A=S\Lambda S^{-1}\)<br>
\(\Rightarrow A^2=S\Lambda (S^{-1}S)\Lambda S^{-1}\)<br>
\(\Rightarrow A^2=S\Lambda \mathcal{I}\Lambda S^{-1}\)<br>
\(\Rightarrow A^2=S\Lambda \Lambda S^{-1}\)<br>
\(\Rightarrow A^2=S\Lambda^2 S^{-1}\)<br>
where,
\(\Lambda^2=\begin{bmatrix}
 \lambda_1^2 &     0     & \cdots  &    0    \\  
 0         & \lambda_2^2 & \cdots  &    0    \\  
 \vdots    & \vdots    & \ddots  & \vdots    \\  
 0         &    0      & \cdots  &    \lambda_n^2    \\  
\end{bmatrix}\)
</blockquote>
<br>
Now we can get formula for \(k^{\text{th}}\) power of \(A\).<br>

<div class="math-border">
    \[
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        \displaystyle A^k = S\Lambda^k S^{-1}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
    \]
</div><br><blockquote>
So <i>eigenvectors</i> of \(A^k\) remains same as the <i>eigenvectors</i> of \(A\).<br>
But <i>eigenvalues</i> of \(A^k\) becomes \(\lambda^k\).<br>
</blockquote>
So now we can find power of matrices quickly, if we have the eigenvectors<i>(independent)</i> of that matrix.<br>
<br>

When we call a matrix \(A\) <b>stable</b>?<br>
<blockquote>
A matrix \(A\) is <b>stable</b> if \(A^k\to 0\) as \(k\to\infty\).<br>
And \(A^k\to 0\) as \(k\to\infty\) <b>if</b>,<br>
\(|\lambda_i| \lt 1;\quad\) \(\forall\ i\in\{1,2,\cdots n\}\)
</blockquote><br>

When a matrix \(A\) is <b>Diagonalizable?</b><br>
<blockquote>
<b>Diagonalizable</b> matrix has \(n\) <b>independent eigenvectors</b>.<br>
So \(A\) is <b>diagonalizable</b> if, all \(\lambda\)'s are different.<br>
\(\lambda_i\neq\lambda_j;\quad \forall \ i\neq j\)<br>
</blockquote><br>

<blockquote>
If all the <i>eigenvalues</i> are different then all the <i>eigenvectors</i> are <b>independent</b>.<br>
</blockquote>
<br>

<blockquote class="noborder">
<b>But</b> reverse is not true, <b>independence</b> of <i>eigenvectors</i> does <b>not</b>
implies that all <i>eigenvalues</i> are different.<br>
<b>Example</b><br>
Say \(A=\mathcal{I}_{3\times 3}=\begin{bmatrix}
 1      & 0 &    0    \\  
 0      & 1 &    0    \\  
 0      & 0 &    1 \\  
\end{bmatrix}\)<br>
So when we apply transformation of \(\mathcal{I}\) nothing changes so every vector is an eigenvector for \(\mathcal{I}\).<br>
But there is only <b>one</b> <i>eigenvalue</i> and that is \(1\).
</blockquote><br>
<br>

<h2><span class="bb">\(\vec{u}_{k+1}=A\vec{u}_k\)</span></h2>
Say we have a vector \(\vec{u}_0\) and a matrix(<b>non-singular</b>) \(A\), with \(n\) <b>independent</b> <i>eigenvectors</i>.<br>
We iteratively apply transformation \(A\) on \(\vec{u}_0\).<br>
\[\vec{u}_{k+1}=A\vec{u}_k\]
\(\vec{u}_{1}=A\vec{u}_0\)<br>
\(\vec{u}_2=A^2\vec{u}_0\)<br>
\(\vec{u}_k=A^k\vec{u}_0\)<br>
<br>
<b>How can we find \(\vec{u}_k\) for an arbitrary large \(k\).</b><br>
\(\vec{u}\) lives in the column space of \(A\), and \(A\) has \(n\) <b>independent</b> <i>eigenvectors</i>
so we can say that \(\vec{u}\) lives in the vector space spanned by these <i>eigenvectors</i>.<br>
so \(\vec{u}\) is the <b>linear combination</b> of these <i>eigenvectors</i>.<br>
say the <i>eigenvectors</i> of \(A\) are \(\vec{x}_1,\vec{x}_2,\cdots,\vec{x}_n\) 
and <i>eigenvalues</i> of \(A\) are \(\lambda_1,\lambda_2,\cdots,\lambda_n\) <br>
So we can say that,<br>
\(\vec{u}_0=c_1\vec{x}_1+c_2\vec{x}_2+\cdots+c_n\vec{x}_n\)<br>
\(\Rightarrow A\vec{u}_0=c_1A\vec{x}_1+c_2A\vec{x}_2+\cdots+c_nA\vec{x}_n\)<br>
\(\Rightarrow A\vec{u}_0=c_1\lambda\vec{x}_1+c_2\lambda\vec{x}_2+\cdots+c_n\lambda\vec{x}_n\)<br>
\(\Rightarrow A^{k}\vec{u}_0=c_1\lambda^{k}\vec{x}_1+c_2\lambda^{k}\vec{x}_2+\cdots+c_n\lambda^{k}\vec{x}_n\)<br>
\(\Rightarrow A^{k}\vec{u}_0=\Lambda^k S \vec{c}\)<br>
\(\Lambda=\begin{bmatrix}
 \lambda_1 &     0     & \cdots  &    0    \\  
 0         & \lambda_2 & \cdots  &    0    \\  
 \vdots    & \vdots    & \ddots  & \vdots    \\  
 0         &    0      & \cdots  &    \lambda_n    \\  
\end{bmatrix};\quad\)
\(S=\begin{bmatrix}
 \vdots & \vdots & \cdots & \vdots \\  
 \vec{x}_{1} & \vec{x}_{2} & \cdots &  \vec{x}_{n}   \\
 \vdots & \vdots & \cdots & \vdots \\
 \end{bmatrix};\quad\)
\(\vec{c}=\begin{bmatrix} c_1\\c_2\\ \vdots \\c_n \end{bmatrix}\)<br>
So,<br>
<div class="math-border">
    \[
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        \displaystyle \vec{u}_k = A^{k}\vec{u}_0=\Lambda^k S\ \vec{c}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
    \]
</div><br>

<br>

<h4 id="fibonacci">Fibonacci example</h4>
Fibonacci sequence: \(0,1,1,2,3,5,\cdots\)<br>
How can we find \(F_k\) for an arbitrary large \(k\).<br>
<span class="bb">\(F_{k+2}=F_{k+1}+F_k\)</span><br>
Now let's write it in form of <span class="bb">\(\vec{u}_{k+1}=A\vec{u}_k\)</span><br>
say \(\vec{u}_k= \begin{bmatrix} F_{k+1}\\F_k\\ \end{bmatrix}\)<br>
and \(\vec{u}_{k+1}=A\vec{u}_k\)<br>
\(\Rightarrow \vec{u}_{k+1}=A\begin{bmatrix} F_{k+1}\\F_k\\ \end{bmatrix}\)<br>
\(\Rightarrow A=\begin{bmatrix} 1&1\\1&0\\ \end{bmatrix}\)<br>
What are it's <i>eigenvalues</i> and <i>eigenvectors</i>.<br>
<blockquote class="noborder">

\(\text{det}(A - \lambda\mathcal{I})=\begin{vmatrix}
        1-\lambda & 1 \\
        1 & -\lambda \\
        \end{vmatrix}\)<br>
\(\text{det}(A - \lambda\mathcal{I})=\lambda^2-\lambda-1\)<br>
for \(\text{det}(A - \lambda\mathcal{I})=0\) we get,<br><br>
<span class="bb" style="padding-bottom: 20px; padding-top: 20px;">\(\lambda_1 = \frac{1+\sqrt{5}}{2}\)</span><br><br>
<span class="bb" style="padding-bottom: 20px; padding-top: 20px;">\(\lambda_2 = \frac{1-\sqrt{5}}{2}\)</span><br><br>
We get our <i>eigenvalues</i> \(\lambda_1, \lambda_2\).<br>
\((A-\lambda\mathcal{I})\vec{x}=0\)<br>
\(\begin{bmatrix}
        1-\lambda & 1 \\
        1 & -\lambda \\
        \end{bmatrix}
        \vec{x}=0
\)<br>
by solving it we get, <br>
\(\vec{x}=\begin{bmatrix} \lambda\\ 1 \end{bmatrix}\)<br>
So our <i>eigenvectors</i> are 
\(\vec{x}_1=\begin{bmatrix} \lambda_1\\ 1 \end{bmatrix},\quad\)
\(\vec{x}_2=\begin{bmatrix} \lambda_2\\ 1 \end{bmatrix}\)<br>
</blockquote><br>
Now we get our <i>eigenvectors</i> and <i>eigenvalues</i>.<br>
\(\vec{u}\) lives in the vector space spanned by these <i>eigenvectors</i>.<br>
so \(\vec{u}\) is the <b>linear combination</b> of these <i>eigenvectors</i>.<br>
we found the <i>eigenvectors</i> of \(A\) \(\vec{x}_1,\vec{x}_2\)
and <i>eigenvalues</i> of \(A\) are \(\lambda_1,\lambda_2\)<br>
So we can say that,<br>
\(\vec{u}_0=c_1\vec{x}_1+c_2\vec{x}_2\)<br>
\(\Rightarrow A\vec{u}_0=c_1A\vec{x}_1+c_2A\vec{x}_2\)<br>
\(\Rightarrow A\vec{u}_0=c_1\lambda_1\vec{x}_1+c_2\lambda_2\vec{x}_2\)<br>
So,<br>
<blockquote>
\[\vec{u}_k=A^{k}\vec{u}_0=c_1\left(\frac{1+\sqrt{5}}{2}\right)^{k}\vec{x}_1+c_2\left(\frac{1-\sqrt{5}}{2}\right)^{k}\vec{x}_2\]
</blockquote><br>
Now let's find \(c_1,c_2\).<br>
we know \(\vec{u}_0=\begin{bmatrix} F_{1}\\F_0\\ \end{bmatrix}\)<br>
\(\Rightarrow \vec{u}_0=\begin{bmatrix} 1\\0\\ \end{bmatrix}\)<br>
And we know that \(\vec{u}_0=c_1\vec{x}_1+c_2\vec{x}_2\) so,<br>
\(\vec{u}_0=c_1\begin{bmatrix} \lambda_1\\ 1 \end{bmatrix}+c_2\begin{bmatrix} \lambda_2\\ 1 \end{bmatrix}\)<br>
\(\Rightarrow c_1\begin{bmatrix} \lambda_1\\ 1 \end{bmatrix}+c_2\begin{bmatrix} \lambda_2\\ 1 \end{bmatrix}=\begin{bmatrix} 1\\0\\ \end{bmatrix}\)<br>
Now we got system of equations,<br>
\(c_1\lambda_1 + c_2\lambda_2 = 1\)<br>
\(c_1 + c_2 = 0\)<br>
by solving we get, \(c_1=\frac{1}{\lambda_1-\lambda_2}\) and \(c_2=\frac{-1}{\lambda_1-\lambda_2}\)<br>
\(\lambda_1-\lambda_2 = \frac{1+\sqrt{5}}{2} - \frac{1-\sqrt{5}}{2}=\sqrt{5}\)
we know that \(\vec{u}_k = A^k\vec{u}_0=c_1\lambda_1^k\vec{x}_1+c_2\lambda_2^k\vec{x}_2\)<br>
\(\Rightarrow \vec{u}_k =\frac{1}{\lambda_1-\lambda_2}\left(\lambda_1^k\vec{x}_1-\lambda_2^k\vec{x}_2\right)\)<br>
\(\Rightarrow \vec{u}_k =\frac{1}{\lambda_1-\lambda_2}\left(
            \lambda_1^k\begin{bmatrix} \lambda_1\\ 1 \end{bmatrix}-
            \lambda_2^k\begin{bmatrix} \lambda_2\\ 1 \end{bmatrix}\right)\)<br>
\(\Rightarrow \vec{u}_k =\frac{1}{\lambda_1-\lambda_2}\left(
            \begin{bmatrix} \lambda_1^{k+1}\\ \lambda_1^k \end{bmatrix}-
            \begin{bmatrix} \lambda_2^{k+1}\\ \lambda_2^k \end{bmatrix}\right)\)<br>
We know that, \(\vec{u}_k= \begin{bmatrix} F_{k+1}\\F_k\\ \end{bmatrix}\)<br>
So,<br>
\[F_k=\frac{\lambda_1^k - \lambda_2^k}{\lambda_1-\lambda_2}\]
\(\lambda_1 = \frac{1+\sqrt{5}}{2}\) and \(\lambda_2 = \frac{1-\sqrt{5}}{2}\) so<br>

<div class="math-border">
    \[
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        \displaystyle F_k=\frac{1}{\sqrt{5}}\left(   \left(\frac{1+\sqrt{5}}{2}\right)^k - \left(\frac{1-\sqrt{5}}{2}\right)^k  \right)
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
    \]
</div>
<blockquote>
For large \(k,\quad\) \(\left(\frac{1-\sqrt{5}}{2}\right)^{k}\approx 0\)<br>
So 
\[F_k\approx \frac{1}{\sqrt{5}}\left(   \left(\frac{1+\sqrt{5}}{2}\right)^k \right)\]
\(\frac{1+\sqrt{5}}{2}\) is known as <a style="color:#185EE0" href="https://en.wikipedia.org/wiki/Golden_ratio">Golden Ratio</a>
</blockquote><br>
<!--
Symmetric matrix will give us pure Real eigenvalue, and
Anti Symmetric matrix will give us pure Complex eigenvalue, and
-->
</p>

<br>
<div class="slack-discuss" onclick=" window.open('https://join.slack.com/t/quantml-org/shared_invite/zt-hcmbg7fr-jPbVAUT_tjGPaKWU50qMYQ','_blank')">
						    Join our Slack <img src="/data/img/slack.png" alt="Slack"> discussion forum
						</div>
<br>

<div id="btn-container">
    <button id="prev-btn" class="button" onclick="window.location.href = '/la/eigenvalues-eigenvectors/';">&#x25C0;&nbsp;&nbsp;Eigenvalues and Eigenvectors</button>
    <button id="next-btn" class="button"  style="float: right;" onclick="window.location.href = '/la/differential-equation/';">Differential Equation&nbsp;&nbsp;&#x25B6;</button>
</div><br>
                        </section>
                    </div>
            </div>

        <!-- Scripts -->
			<script src="/data/assets/js/jquery.min.js"></script>
			<script src="/data/assets/js/jquery.scrollex.min.js"></script>
			<script src="/data/assets/js/jquery.scrolly.min.js"></script>
			<script src="/data/assets/js/browser.min.js"></script>
			<script src="/data/assets/js/breakpoints.min.js"></script>
			<script src="/data/assets/js/util.js"></script>
            <script src="/data/assets/js/main.js"></script>
            <script src="/data/index.js"></script>
			<script src="/data/prism/prism.js"></script>
            <script>setTimeout(() => {document.querySelector('body').style.display="block"}, 0)</script>

    </body>
</html>