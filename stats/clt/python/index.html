<!DOCTYPE HTML>
<html lang="en">

	<head>
		<script>
			var nextPage = "/stats/hoeffdings-inequality/";
			var prevPage = "/stats/lln/"
			var nextPageTitle = "Hoeffdings Inequality";
			var prevPageTitle = "Law of Large Numbers"
			var arrowLeftPage = "/stats/clt/"
			var arrowRightPage = "/stats/clt/julia/"
		</script>

		<link rel="stylesheet" href="/data/assets/css/main.css">
		<link rel="stylesheet" href="/data/assets/css/authStyle.css">

		<title>Python | Central Limit Theorem | Fundamentals of Statistics - QuantML</title>
		<meta charset="utf-8" />
		<link rel="icon" href="/data/icon.png" type="image/png" sizes="16x16">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="robots" content="index, follow">

		<!--Prism-->
		<link rel="stylesheet" href="/data/prism/prism.css">

	<script src="/data/assets/js/initialize.js"></script>
		<script src="/data/assets/js/swiped-events.js"></script>

	</head>

	<body>

		<!-- Wrapper -->
			<div style="display: none;" id="wrapper">
				<div class="bg fixed" style="transform: none;"></div>
				<div id="navPanelToggle">Menu</div>


				<!-- Header -->
				<header id="header">
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-light"><img src="/data/img/cover.webp" alt="QuantML.org" /></a>
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-dark"><img src="/data/img-dark/cover.webp" alt="QuantML.org" /></a>
					<script>
						if(localStorage.getItem("quantmlTheme")=="dark") document.getElementById('quantml-cover-dark').style.display = "block"
						else document.getElementById('quantml-cover-light').style.display = "block"
					</script>
				</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="cover"><a href="/stats/"><img src="/data/img/STATISTICS-cover.png" alt="STATISTICS"></a></li>
							<li class="link"><a href="/stats/clt/">Central Limit Theorem</a></li>
							<li class="active python"><img src="/data/img/python.png" alt="Python"></li>
							<li class="julia"><a href="/stats/clt/julia/"><img src="/data/img/julia.png" alt="Julia"></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<section class="post">

<div style="text-align: center;" id="load-init"></div>
<script>initializeBody();</script>

<div id="paragraph-content" data-swipe-threshold="80">

<div id="desktop-mode">
	<div id="btn-container"></div><br>
</div>

<blockquote class="sidebar">
	First let's see how we can perform a <b>single simulation</b>, then we will see how to perform <b>multiple simulations</b> and <b>visualize</b> the Weak Law of Large Numbers.
</blockquote><br>

<h1>Single Simulation <img src="/data/img/python.png" alt="Python" width="112.5px" height="30px"></h1>

Assume that there are \(1000\) students in your school and you want to know, what is the average height of \(100\) randomly selected students? and how is that average height is distributed (or say the distribution of that average height)?<br>

<a href="/stats/introduction/#dogma">Remember that dogma</a> <br>
<img id='dogma' class="full-size-img" src="/stats/img/dogma.png" alt="Central Dogma of Probability and Statistics">

<h2>Truth</h2>
Now let's define the <b>truth</b>. <br>
Say that every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a> with range of \([165\text{ cm },185\text{ cm}]\). <br>
So \(X_1,\cdots,X_{50}\sim\text{Unif}[165, 185]\) <br>
<blockquote class="noborder">
	<b>Note:</b> this information is completely unknown to us.
</blockquote>
<br>

<h2>Probability</h2>
We use probability to generate data using the <b>Truth</b> we defined above.<br>
Now let's create the population of all \(1000\) students. <br>

<pre data-start="1"><code class="language-python line-numbers">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform # to draw random variables from Uniform distribution

N = 1000        # population size
a = 165
scale = 20

# Unif[a, a + scale]
distribution = uniform(a, scale)
population = distribution.rvs(N)
</code></pre>
<br>

<h2>Observation</h2>
Now that we have all \(1000\) students, and every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a>. <br>
Let's (randomly) take a sample of \(100\) students
<pre data-start="11"><code class="language-python line-numbers">n = 100
sample = np.random.choice(population,n)
</code></pre>
<br>

<h2>Statistics</h2>
So now we have our sample of \(100\) students, let's estimate the average height of those \(100\) students <b>(sample mean)</b>. <br>

<pre data-start="13"><code class="language-python line-numbers">sample_mean = np.mean(sample)
</code></pre>

<span class="bb">sample_mean</span> is the average height of \(100\) randomly selected students. <br>

Now let's see how the distribution of the average height of \(100\) randomly selected students looks like. <br>
To achieve it we need to perform this simulation multiple times and plot all sample means as a histogram. <br>
<br>

<h1>Multiple simulations <img src="/data/img/python.png" alt="Python" width="112.5px" height="30px"></h1>
The <b>truth</b> is that every student's height follows the <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">Uniform distribution</a> \(\text{Unif}[165, 185]\), so let's start with creating population. <br>

<pre data-start="1"><code class="language-python line-numbers">import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform # to draw random variables from Uniform distribution
np.random.seed(1)

N = 1000        # population size
a = 165
scale = 20

# Unif[a, a + scale]
distribution = uniform(a, scale)
population = distribution.rvs(N)
</code></pre>
<br>

Now let's collect <span class="bb">n_simulations</span> sample means of sample size \((n) = 30\). <br>
(say n_simulations = 200)
<pre data-start="12"><code class="language-python line-numbers">n = 30
n_simulations = 200
sample_means = np.zeros(n_simulations)
for k in range(n_simulations):
	sample_means[k] = np.mean(np.random.choice(population,n))
</code></pre>
Now let's plot these sample means. <br>

<pre data-start="17"><code class="language-python line-numbers">num_bins = 50
n, bins, patches = plt.hist(sample_means, num_bins, facecolor='blue', alpha=0.5)
plt.xlabel("Sample mean")
plt.ylabel("# occurrences of sample mean")
plt.show()
</code></pre>

<img class="full-size-img" src="/stats/img/clt/clt-sampeling-distribution.png" alt="">

<div style="text-align: center;">Sample mean seems to be normally distributed.</div>
Well it's a Central Limit Theorem Simulation right, and whole point of CLT is to approximate probability distribution of sample mean \(\overline{X}_n\) to it's corresponding Normal distribution <b>am I correct?</b> <br>
Let's see ourself, let's overlay a Normal distribution with true mean and variance of <span class="bb">sample_means</span>. <br>

<pre data-start="17"><code class="language-python line-numbers">from scipy.stats import norm
samples_mean = np.mean(sample_means)
samples_std = np.std(sample_means)

fig, ax = plt.subplots()

# Getting PDF
x = np.linspace(samples_mean - 3*samples_std, samples_mean + 3*samples_std, 100)
pdf = norm.pdf(x, samples_mean, samples_std)
ax.plot(x, pdf, c="g", label="Normal distribution (PDF)")

# Getting Histogram
num_bins = 50
counts, bins = np.histogram(sample_means, 
							np.linspace(min(sample_means), 
										max(sample_means), 
										num_bins)
							)
n, bins, patches = ax.hist(sample_means, 
							bins[:-1], 
							facecolor='blue', 
							alpha=0.5,
							color="b",
							weights=(1/sum(counts))*np.ones_like(sample_means),
							label="Sampling distribution")
leg = ax.legend();
</code></pre>
<img class="full-size-img" src="/stats/img/clt/clt-sim-pdf.png" alt="">
Here height of the bins of Sampling distribution represent the probability of falling into that bin. <br>
The Normal distribution that we plotted have same <b>mean</b> and <b>variance</b> as of our Sampling distribution, but there shapes are way off. <br>

<br>
<details>
	<summary><i><span class="bb">Sampling distribution of \(\sqrt{n}\, (\overline{X}_n-\mu)\) is much better fit for Normal distribution</span></i></summary>
<blockquote class="noborder">
	If you follow the result of the CLT,
	\[\sqrt{n}\, (\overline{X}_n-\mu) \xrightarrow [n\to \infty ]{(d)} \mathcal{N}(0,\sigma^2) \]
	Then you can see our <b>normally distributed</b> data. <br>
	We just have to take the <b>true mean</b> instead of taking average of all sample means, <b>true variance</b> instead of taking variance of all of our sample means, center our observations around \(0\) and then stretch it by a factor of \(\sqrt{n}\). <br>
	(this is also what you will see in most of the simulations online). <br>
	<br>
	<h4>So why we are using mean and variance of our sampling distribution instead of <b>true</b> mean and variance?</h4>
	<details>
		<summary><i><span class="bb">Answer</span></i></summary>
		<blockquote class="sidebar">
			We are using the estimated mean and variance of our sampling distribution instead of <b>true</b> mean and variance because, in reality we just have <b>data</b> we don't know the underlying random process, so we don't know the <b>true mean</b> and <b>true variance</b>. <br>
			In this simulation we can easily get true mean by <span class="bb">distribution.mean()</span> and true variance by <span class="bb">distribution.var()</span> but this can't be done in real dataset.
		</blockquote>
	</details>
	<pre data-start="17"><code class="language-python line-numbers">from scipy.stats import norm
mean = distribution.mean() # True mean
std = distribution.std() # True standard deviation

# Centered and stretched sample mean (r.v.)
sample_means = np.sqrt(n) * np.array(sample_means - mean)

fig, ax = plt.subplots()

# Getting PDF N(0, Ïƒ)
x = np.linspace(- 3*std, 3*std, 100)
pdf = norm.pdf(x, 0, std)
ax.plot(x, pdf, c="g", label="Normal distribution (PDF)")

# Getting Histogram
num_bins = 50
counts, bins = np.histogram(sample_means, 
							np.linspace(min(sample_means), 
										max(sample_means), 
										num_bins)
							)
n, bins, patches = ax.hist(sample_means, 
							bins[:-1], 
							facecolor='blue', 
							alpha=0.5,
							color="b",
							weights=(1/sum(counts))*np.ones_like(sample_means),
							label="Sampling distribution")
leg = ax.legend();
</code></pre>
<img class="full-size-img" src="/stats/img/clt/clt-centered.png" alt="">
Now the plot feels much more <b>Normal</b> but here the random variable is \(\sqrt{n}\, (\overline{X}_n-\mu) \) not \(\overline{X}_n\). <br>

<br>
</blockquote>
</details>
<br>

And as we <a href="/stats/clt/#its-about-CDF">discussed earlier</a> that,
<blockquote class="sidebar">
	<b>Central Limit Theorem</b> is <b>not</b> a statement about the convergence of PDF or PMF. It's a statement about the convergence of <b>CDF</b>.
</blockquote>

Now let's see ourself,

<pre data-start="44"><code class="language-python line-numbers"># Plot CDF 
fig, ax = plt.subplots()
# CDF of Normal Distribution
cdf = norm.cdf(x, samples_mean, samples_std)
ax.plot(x, cdf, c='g', label="Normal distribution CDF")

# CDF of Sampling Distribution
cdf = np.cumsum(n)/sum(n)
ax.scatter(bins[:-1], cdf, c='b', marker="*", label="Sampling distribution CDF")
leg = ax.legend();
plt.show()
</code></pre>

<img class="full-size-img" src="/stats/img/clt/clt-sim-cdf.png" alt="">
Here we can see the beauty of <b>Central limit theorem</b>, we can see how close the CDF of our Sampling distribution get closer to the CDF of a Normal distribution. <br>

Now try tweaking <span class="bb">\(n\)</span> and see how it affect the CDF convergence. <br>
Also try different distributions form <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy.stats</a> <br>

<blockquote class="noborder align-center">
	<a href="/stats/clt/julia/"><img src="/data/img/julia.png" alt="Julia" width="53px" height="33px"> Simulation</a>
	<br>
    <a target="=_blank" href="https://app.quantml.org/stats/?ch=CLT&dist=Uniform+distribution">Visualize Central Limit Theorem &nbsp;<img src="/data/img/app.png" alt="launch" width="30px" height="30px"></a> <br>
    Try different distributions, tweak there parameters, sample size, number of simulations and see how it impacts the convergence of <b>CDF</b> of \(\overline{X}_n\).
</blockquote><br>


<div class="slack-discuss" onclick=" window.open('https://join.slack.com/t/quantml-org/shared_invite/zt-jffw86bo-6M260iyt1q2MgBma9elewg','_blank','noopener')">
	Join our Slack <img class="slack-logo" src="/data/img/slack.png" alt="Slack"> discussion forum
</div><br>

</div>

<div id="btn-container"></div><br>


<div id="modals-html"></div>

					</section>
					</div>
			</div>


		<!-- Scripts -->
		<script src="/data/assets/js/script.js"></script>
		<script>cssLoaded()</script>
		<script src="/data/prism/prism.js"></script>

	</body>
</html>