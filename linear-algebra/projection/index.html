<!DOCTYPE HTML>
<html lang="en">
    <head>

        <script>
            var isConcluded = true;
            var arrowLeftPage = "/linear-algebra/orthogonal-subspaces/"
            var arrowRightPage = "/linear-algebra/least-squares/"
        </script>

        <link rel="stylesheet" href="/data/css/main.css">
                
		<title>Projection - QuantML</title>
		<meta charset="utf-8" />
        <link rel="icon" href="/data/icon.png" type="image/png" sizes="16x16">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="robots" content="index, follow">
        <meta name="author" content="Yuvraj Garg">

        <script src="/data/js/initialize.js"></script>

	</head>   
    
    <body>

        <!-- Wrapper -->
			<div style="display: none;" id="wrapper">
                <div class="bg fixed" style="transform: none;"></div>
                <div id="navPanelToggle">Menu</div>


				<!-- Header -->
				<header id="header">
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-light"><img src="/data/img/cover.webp" alt="QuantML.org" /></a>
					<a href="https://quantml.org" style="display: none;" class="logo image jump-big" id="quantml-cover-dark"><img src="/data/img-dark/cover.webp" alt="QuantML.org" /></a>
					<script>if(window.quantml["theme"]=="dark") document.getElementById('quantml-cover-dark').style.display = "block";else document.getElementById('quantml-cover-light').style.display = "block";</script>
				</header>

                <!-- Nav -->
                    <nav id="nav">
                        <ul class="links">
                            <li class="cover"><a href="/linear-algebra/"><img src="/data/img/linear-algebra-cover.png" alt="Linear Algebra"></a></li>
                            <li class="active title">Projection</li>
                            <li><a href="/linear-algebra/least-squares/">Least Squares(Application of projection)</a></li>
                        </ul>
                        <ul id="nav-bar-icons-head" class="icons"></ul>
                    </nav>

<!-- Main -->
<div id="main">
    <section class="post">
<div style="text-align: center;" id="load-init"></div>
<script>initializeBody();</script>

<div id="paragraph-content" data-swipe-threshold="80">
<div id="btn-container">
    <button id="prev-btn" class="button" onclick="window.location.href = '/linear-algebra/orthogonal-subspaces/';">&#x25C0;&nbsp;&nbsp;Orthogonal Subspaces</button>
    <button id="next-btn" class="button"  style="float: right;" onclick="window.location.href = '/linear-algebra/least-squares/';">Least Squares&nbsp;&nbsp;&#x25B6;</button>
</div><br>


<p>
<!-- <h1>Projection</h1> -->

<h1 id="vec-proj">Vector Projection</h1>

<img class="full-size-img" src="/linear-algebra/img/proj-0.webp" width="184" height="136" alt="" style="float:right">
Say that we have two vector <span class="bb">\(\vec{v}\in\mathbb{R}^2\)</span> and <span class="bb">\(\vec{u}\in\mathbb{R}^2\)</span>.<br>
Now <b>in space of \(\vec{v}\)</b> which vector is <b>closest</b> to \(\vec{w}\)?<br>
But let's first ask what is the space of \(\vec{v}\)?<br>

<br>
<blockquote>    
    We are asking for the space of <b>a single vector</b>, a single vector can only give us a
    space in \(\mathbb{R}\) which is a <b>line</b>.<br>
    So the <b>span</b> of \(\vec{v}\) is \(C\vec{v};\quad C\in\mathbb{R}\) 
</blockquote>
Now we have to find a vector <b>closest</b> to \(\vec{w}\) in the <b>vector space</b> of \(\vec{v}\).<br>
Let's call this <b>closest</b> vector as \(\vec{p}\), 
this \(\vec{p}\) is the projection of \(\vec{w}\) on \(\vec{v}\).<br>
We know that \(\vec{p}\) lives in the <b>vector space</b> of \(\vec{v}\).<br>
So <span class="bb">\(\vec{p}=x\vec{v};\quad x\in\mathbb{R}\)</span>.<br>
\(\vec{p}\) is the <b>closest</b> vector to \(\vec{w}\) then the vector joining 
\(\vec{p}\) to \(\vec{w}\) is perpendicular to \(\vec{v}\).<br>
Say the vector joining \(\vec{p}\) to \(\vec{w}\) be \(\vec{e}\), and \(\vec{e} = \vec{w}-\vec{p}\)
So,<br>
<blockquote class="noborder">
\(\vec{v}\cdot \vec{e} =0\)<br>
\(\Rightarrow \vec{v}^T \vec{e} =0\)<br>
\(\Rightarrow \vec{v}^T (\vec{w} -\vec{p})=0\)<br>
\(\Rightarrow \vec{v}^T (\vec{w} -x\vec{v})=0\)<br>
\(\Rightarrow \vec{v}^T \vec{w} -x\vec{v}^T\vec{v}=0\)<br>
</blockquote>
\[\Rightarrow x=\frac{\vec{v}^T \vec{w}}{\vec{v}^T\vec{v}} \in\mathbb{R}\]
<b>Projection</b> of \(\vec{w}\) on \(\vec{v}\) is <span class="bb">\(\vec{p}=\vec{v}x\)</span><br>
<br>
<h1 id="vector-projection-matrix">Projection Matrix of a vector</h1>
Say we have a vector <span class="bb">\(\vec{v}\in\mathbb{R}^n\)</span> now we want a <b>matrix</b> that gives us projection 
of <b>any</b> vector onto this vector \(\vec{v}\), we call this matrix a <b>projection matrix</b>.<br>
Say this projected vector on \(\vec{v}\) be \(\vec{p}\).<br>
As we discussed above,<br>
\(\vec{p}=\vec{v}x\)<br>
\(\displaystyle \Rightarrow \vec{p}= \vec{v} \frac{\vec{v}^T \vec{w}}{\vec{v}^T\vec{v}}\)<br>
\(\displaystyle \Rightarrow \vec{p}= \frac{\vec{v} \vec{v}^T}{\vec{v}^T\vec{v}} \vec{w}\)<br>
<br>
Projection matrix\((P)\) of a vector \(\vec{v}\) is

<div class="l1 border">
\[    
    \begin{matrix}

    % equation
    \displaystyle P=\frac{\vec{v}\ \vec{v}^T}{\vec{v}^T\vec{v}} 
    %equation

    \end{matrix}
\]
</div>

<li>Columns of \(P\) is the <b>linear combinations</b> of \(\vec{v}\), so </li>
<blockquote>    
Column space of \(P\) is the vector space of \(\vec{v}\).<br>
</blockquote>

<li>We have our <b>Projection matrix</b>\((P)\) we can take <b>projection</b> of any vector 
\(\vec{w}\) on \(\vec{v}\) by evaluating \(P\vec{w}\)</li>
<blockquote>    
In \(P\vec{w}\) we are taking the <b>linear combination</b> of \(\vec{v}\) so the resultant 
projection of \(\vec{w}\) on \(\vec{v}\) lives in the <b>vector space</b> of \(\vec{v}\).<br>
</blockquote>

<li>What about the <b>rank</b> of \(P\)?</li>
<blockquote>    
Rank of \(P\) is \(1\).<br>
Because all the columns of \(P\) is the <b>linear combination</b> of a <b>single vector</b> \(\vec{v}\).
</blockquote>

<li>Is \(P\) <b>symmetric</b>?</li>
\(\displaystyle P= \frac{\vec{v} \vec{v}^T}{\vec{v}^T\vec{v}} \)<br>
\(\displaystyle \Rightarrow P^T = \frac{(\vec{v} \vec{v}^T)^T}{\vec{v}^T\vec{v}} \)<br>
\(\displaystyle \Rightarrow P^T = \frac{\vec{v}^{T^T}  \vec{v}^T}{\vec{v}^T\vec{v}} \)<br>
\(\displaystyle \Rightarrow P^T = \frac{\vec{v} \vec{v}^T}{\vec{v}^T\vec{v}} \)<br>
So \(P\) is symmetric.
<blockquote>    
\[P=P^T\]
</blockquote>

<li>What if we apply the projection of a vector \(\vec{w}\) on \(\vec{v}\) <b>twice</b>?</li>
When we apply the <b>projection</b> first time, we land in the <b>vector space</b> of \(\vec{v}\).<br>
Let's say the projected vector be \(\vec{p}_1\).<br>
If we again apply the <b>projection</b> on \(\vec{p}_1\) then it project the \(\vec{p_1}\) 
into the vector space of \(\vec{v}\).<br>
But \(\vec{p}_1\) is already in <b>vector space</b> of \(\vec{v}\), so the second projection did nothing.<br>
So we can say that 
<blockquote>    
\[P^2=P\]
</blockquote><br>
<br>
<h1 id="why-project">Why are we doing projection?</h1>
Ok we can find the projection of a vector on another vector, <b>but why</b> are we projecting the
vector in the first place?<br>
<b>What is the benefit of projecting a vector?</b><br>
<span id="example"></span>
Say a function \(f=x_1 a+ x_2 b+ x_3 c\) is governed by 3 variables (say \(a,b,c\)) and we have \(5\) <b>noisy</b>
observation of \(f\) now we want to predict \(f\) for some set of \((a,b,c)\).<br>
Because our \(5\) observations are <b>noisy</b>  so we <b>can not</b> perfectly predict the outcome.<br>
Our observation is something like,<br>
\(f_1=x_1 a_1+ x_2 b_1+ x_3 c_1\)<br>
\(f_2=x_1 a_2+ x_2 b_2+ x_3 c_2\)<br>
\(f_3=x_1 a_3+ x_2 b_3+ x_3 c_3\)<br>
\(f_4=x_1 a_4+ x_2 b_4+ x_3 c_4\)<br>
\(f_5=x_1 a_5+ x_2 b_5+ x_3 c_5\)<br>
We can write it as,<br>
\(\begin{bmatrix}
a_1 & b_1 & c_1\\
a_2 & b_2 & c_2\\
a_3 & b_3 & c_3\\
a_4 & b_4 & c_4\\
a_5 & b_5 & c_5\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix}
=
\begin{bmatrix}
f_1\\
f_2\\
f_3\\
f_4\\
f_5\\
\end{bmatrix}
\)<br>

Here \(x_1 , x_2 , x_3\) are our parameters(unknown), so our parameter
space is \(\mathbb{R}^5\).<br>
We want to find a \(4\) dimensional plane that best fit our <b>noisy</b> data.<br>
Say
\(\begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ a_4 \\ a_5 \\ \end{bmatrix} = \vec{a},\quad\)
\(\begin{bmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \\ b_5 \\ \end{bmatrix} = \vec{b},\quad\)
\(\begin{bmatrix} c_1 \\ c_2 \\ c_3 \\ c_4 \\ c_5 \\ \end{bmatrix} = \vec{c},\quad\)

 \(\begin{bmatrix}
a_1 & b_1 & c_1\\
a_2 & b_2 & c_2\\
a_3 & b_3 & c_3\\
a_4 & b_4 & c_4\\
a_5 & b_5 & c_5\\
\end{bmatrix}= \mathbb{A}, \quad\)
\(\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix}=\mathbb{X},\quad\)
and 
\(\begin{bmatrix}
f_1\\
f_2\\
f_3\\
f_4\\
f_5\\
\end{bmatrix}
=\mathbb{Y}\)<br>
So <span class="bb">\(\mathbb{A}\mathbb{X}=\mathbb{Y}\)</span><br>
Our data is <b>noisy</b>.<br>
<span class="bb">\(\text{Rank}(\mathbb{A})\leq 3\)</span>,
<span class="bb"> \(\mathbb{X}\in\mathbb{R}^3\)</span>,
<span class="bb"> \(\mathbb{Y}\in\mathbb{R}^5\)</span>.<br>
So there are <b>high chance</b> that \(\mathbb{Y}\) does <b>not</b> lives in <b>column space</b> of \(\mathbb{A}\).<br> 
So <b>instead</b> we find the solution for <span class="bb">\(\mathbb{A}\widehat{\mathbb{X}}=\widehat{\mathbb{Y}}\)</span>.<br>
Where \(\widehat{\mathbb{Y}}\) lives in the column space of \(\mathbb{A}\) <b>and</b> \(\widehat{\mathbb{Y}}\)
is <b>closest</b> to \(\mathbb{Y}\)
<blockquote>    
We can get this \(\widehat{\mathbb{Y}}\) by taking the <b>projection</b> of \(\mathbb{Y}\)
onto the <b>column space</b> of \(\mathbb{A}\)
</blockquote><br>

<h1 id="matrix-projection">Matrix Projection</h1>
Say that we have a \(m\times n\) matrix <span class="bb">\(\mathbb{A}\in\mathbb{R}^{m\times n}\)</span> and a vector 
<span class="bb">\(\vec{v}\in\mathbb{R}^n\)</span>.<br>
Then <b>projection</b> of a vector \(\vec{v}\) on a matrix \(\mathbb{A}\) <i>generally</i> means <b>projection</b>
of \(\vec{v}\) on the <b>column space</b> of matrix \(\mathbb{A}\).<br>
We can also take the <b>projection</b> of a vector \(\vec{v}\) on the <b>null space</b> of matrix \(\mathbb{A}^T\).<br>
And as we know that <b>null space</b> of \(\mathbb{A}^T\) is <b>perpendicular</b> to the <b>column space</b> of \(\mathbb{A}\).<br>
Then,

<blockquote>
Projection of a vector \(\vec{v}\) on the,<br>
<li><b>null space</b> of matrix \(\mathbb{A}^T\), and</li>
<li><b>column space</b> of matrix \(\mathbb{A}\)</li>
collectively gives us back our vector \(\vec{v}\)
</blockquote><br>

<h2 id="matrix-projection-column">Projection onto the column space of a matrix</h2>
Recall our <a  href="/linear-algebra/projection/#example">previous example</a>, 
here we have a system of equation <span class="bb">\(\mathbb{A}\mathbb{X}=\mathbb{Y}\)</span> where there is <b>no solution</b>.<br>
Here problem was that there are <b>high chance</b> that \(\mathbb{Y}\) does <b>not</b> lives in <b>column space</b> of \(\mathbb{A}\), 
So it does <b>not</b> have a solution.<br> 
So we go for the <b>closest</b> solution.<br>
<span class="bb">\(\mathbb{A}\widehat{\mathbb{X}}=\widehat{\mathbb{Y}}\)</span><br>
where, \(\widehat{\mathbb{Y}}\) lives in the <b>column space</b> of \(\mathbb{A}\), so.<br>
<!--(And we put a <b>hat</b> on\(\mathbb{X}\) just to show that it's the estimated \(\mathbb{X}\)).<br>-->
<blockquote>
\(\widehat{\mathbb{Y}}\) is the <b>linear combinations</b> of the columns of \(\mathbb{A}\).
</blockquote>
(vector)\(\widehat{\mathbb{Y}}\) lives in the column space \(\mathbb{A}\) <b>and</b> 
(vector)\(\mathbb{Y}\) is at some angle to the column space of \(\mathbb{A}\), So <br>
<blockquote>
The vector \(\mathbb{Y} - \widehat{\mathbb{Y}}\) is perpendicular to the <b>column space</b> of \(\mathbb{A}\)
</blockquote>
\(\Rightarrow \mathbb{Y} - \widehat{\mathbb{Y}}\) is also perpendicular to the column <b>vectors</b> of \(\mathbb{A}\)<br>
\(\Rightarrow\) <span class="bb">\(\vec{a}^T (\mathbb{Y} - \widehat{\mathbb{Y}})=0\)</span>, 
<span class="bb">\(\vec{b}^T (\mathbb{Y} - \widehat{\mathbb{Y}})=0\)</span> and
<span class="bb">\(\vec{c}^T (\mathbb{Y} - \widehat{\mathbb{Y}})=0\)</span>
where \(\vec{a},\) \(\vec{b}\) and \(\vec{c}\) are the column vector of \(\mathbb{A}\).<br>
We can write it as,<br>

<div class="l1 border">
\[    
    \begin{matrix}

    % equation
    \displaystyle \mathbb{A}^T(\mathbb{Y} - \widehat{\mathbb{Y}})=0
    %equation

    \end{matrix}
\]
</div>

Notice that \(\mathbb{Y} - \widehat{\mathbb{Y}}\) lives in \((N(A^T))\)
<li>The <a  href="/linear-algebra/fundamental-subspaces/#left-null-space">Left Null Space</a>
of matrix \(\mathbb{A}\) OR say </li>
<li>The <a  href="/linear-algebra/null-space/">Null Space</a> of \(\mathbb{A}^T\).<br></li> 
And <b>Null Space</b> of \(\mathbb{A}^T\) is <b>perpendicular</b> to the <b>column space</b> of \(\mathbb{A}\).<br>
<blockquote>    
So \(\mathbb{Y} - \widehat{\mathbb{Y}}\) is  <b>perpendicular</b> to the <b>column space</b> of \(\mathbb{A}\).<br>
</blockquote>
\(\mathbb{A}^T(\mathbb{Y} - \widehat{\mathbb{Y}})=0\) <b>and</b> \(\widehat{\mathbb{Y}}=\mathbb{A}\widehat{\mathbb{X}}\)<br>
\(\Rightarrow \mathbb{A}^T(\mathbb{Y} - \mathbb{A}\widehat{\mathbb{X}})=0\)<br>
\(\Rightarrow \mathbb{A}^T \mathbb{Y} - \mathbb{A}^T\mathbb{A}\widehat{\mathbb{X}}=0\)<br>
\(\Rightarrow \mathbb{A}^T\mathbb{A}\widehat{\mathbb{X}}=\mathbb{A}^T \mathbb{Y}\)<br>

<div class="l1 border">
\[    
    \begin{matrix}

    % equation
    \displaystyle \widehat{\mathbb{X}}=\left(\mathbb{A}^T\mathbb{A}\right)^{-1}\mathbb{A}^T \mathbb{Y}
    %equation

    \end{matrix}
\]
</div>

Our <b>projection</b> of \(\mathbb{Y}\) on the columnn space of \(\mathbb{A}\) is \(\widehat{\mathbb{Y}}\) .<br>
<span class="bb">\(\widehat{\mathbb{Y}}=\mathbb{A}\widehat{\mathbb{X}}\)</span> and <span class="bb">\(\widehat{\mathbb{X}}=(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T \mathbb{Y}\)</span>, so<br>
<br>
<li>Projection\((\widehat{\mathbb{Y}})\) of \(\mathbb{Y}\) on the <b>column space</b> of \(\mathbb{A}\):</li>

<div class="l1 border">
\[    
    \begin{matrix}

    % equation
    \displaystyle \widehat{\mathbb{Y}}=\mathbb{A}\left(\mathbb{A}^T\mathbb{A}\right)^{-1}\mathbb{A}^T \mathbb{Y}
    %equation

    \end{matrix}
\]
</div>

<br>
So our <b>Projection Matrix</b> is, 

<div class="l1 border">
\[    
    \begin{matrix}

    % equation
    \displaystyle P=\mathbb{A}\left(\mathbb{A}^T\mathbb{A}\right)^{-1}\mathbb{A}^T
    %equation

    \end{matrix}
\]
</div>

<li>Is \(P\) <b>symmetric</b>?</li>
\(\displaystyle P= \mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\)<br>
\(\displaystyle \Rightarrow P^T = \left(\mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\right)^T\)<br>
\(\displaystyle \Rightarrow P^T = \mathbb{A}^{T^T}((\mathbb{A}^T\mathbb{A})^{-1})^T\mathbb{A}^T\)<br>
\(\displaystyle \Rightarrow P^T = \mathbb{A}^{T^T}\left((\mathbb{A}^T\mathbb{A})^T\right)^{-1}\mathbb{A}^T\)<br>
\(\displaystyle \Rightarrow P^T = \mathbb{A}^{T^T}(\mathbb{A}^T\mathbb{A}^{T^T})^{-1}\mathbb{A}^T\)<br>
\(\displaystyle \Rightarrow P^T = \mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\)<br>
So \(P\) is symmetric.
<blockquote>    
\[P=P^T\]
</blockquote><br>

<li>What if we project <b>vector</b> \(\mathbb{Y}\) on the <b>column space</b> of a matrix \(\mathbb{A}\) of a matrix \(\mathbb{A}\) <b>twice</b>?</li>
When we apply the <b>projection</b> first time, we land in the <b>column space</b> of \(\mathbb{A}\).<br>
Let's say the projected vector be \(\widehat{\mathbb{Y}}\).<br>
If we again project the \(\widehat{\mathbb{Y}}\) on the <b>column space</b> of \(\mathbb{A}\) 
then the resultant vector will be \(\widehat{\mathbb{Y}}\).<br>
Because \(\widehat{\mathbb{Y}}\) is already in <b>column space</b> of \(\mathbb{A}\), so the second projection did nothing.<br>
So we can say that 
<blockquote>    
\[P^2=P\]
</blockquote><br>
Let's look at the extreme cases,<br>
Suppose we want to take projection of a vector \(\vec{b}\) in the <b>column space</b> of a matrix \(\mathbb{A}\).<br>

\(1.\) <b>What If the vector \(\vec{b}\) is <b>perpendicular</b> to the <b>column space</b> of matrix \(\mathbb{A}\)?</b>
<dl><dd>
    First let's think about the space of vector<b>s</b> who are <b>perpendicular</b> to the <b>column space</b> of matrix \(\mathbb{A}\).<br>
    Space <b>perpendicular</b> to the <b>column space</b> is <b>Null space</b> of \(\mathbb{A}^T\).<br>
    So \(\vec{b}\) lives in the <b>Null space</b> of \(\mathbb{A}^T\).<br>
    So, 
    <blockquote>    
    \(\mathbb{A}^T\vec{b}=\vec{0}\)
    </blockquote>
    <li><b>projection matrix</b>\((P)\) of the <b>column space</b> of a matrix \(\mathbb{A}\) is,</li>
    <blockquote>    
        \(P= \mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\)        
    </blockquote>
    And <b>projection</b>\(P\) of \(\vec{b}\) in the <b>column space</b> of a matrix \(\mathbb{A}\) is \(P\vec{b}\) <br>
    \(P\vec{b}= \mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\underbrace{\mathbb{A}^T\vec{b}}_{=\vec{0}}=\vec{0}\)
</dd></dl>\(2.\) <b>What If the vector \(\vec{b}\) is <b>in</b> the <b>column space</b> of \(m\times n\) matrix \(\mathbb{A}\)?</b>
<dl><dd>
    <b>Vectors</b> in the <b>column space</b> of \(\mathbb{A}\) is the <b>linear combinations</b> of the column vectors of matrix\(\mathbb{A}\).<br>
    We can write the all the linear combinations of matrix all as \(\mathbb{A}\vec{x};\quad\vec{x}\in\mathbb{R}^n\).<br>
    And we are saying that  \(\vec{b}\) is <b>in</b> the <b>column space</b> of matrix \(\mathbb{A}\), so ,<br>
    <blockquote>
    \(\vec{b}=\mathbb{A}\vec{x};\quad\vec{x}\in\mathbb{R}^n\).
    </blockquote>
    So <b>projection</b>\(P\) of \(\vec{b}\) in the <b>column space</b> of a matrix \(\mathbb{A}\) is \(P\vec{b}\) <br>
    \(P\vec{b}= \mathbb{A}(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\vec{b}\).<br>
    \(P\vec{b}= \mathbb{A}\underbrace{(\mathbb{A}^T\mathbb{A})^{-1}\mathbb{A}^T\mathbb{A}}_{\mathcal{I}(  identity)}\vec{x}\).<br>
    \(P\vec{b}= \mathbb{A}\vec{x}\).<br>
    \(P\vec{b}= \vec{b}\).<br>
    So projecting \(\vec{b}\)(which is already in the column space of \(\mathbb{A}\)) on the the <b>column space</b> of \(\mathbb{A}\)
    changes nothing.<br>
</dd></dl>
</p>



</div>
<div id="btn-container">
    <button id="prev-btn" class="button" onclick="window.location.href = '/linear-algebra/orthogonal-subspaces/';">&#x25C0;&nbsp;&nbsp;Orthogonal Subspaces</button>
    <button id="next-btn" class="button"  style="float: right;" onclick="window.location.href = '/linear-algebra/least-squares/';">Least Squares&nbsp;&nbsp;&#x25B6;</button></div><br>
    <div id="modals-html"></div>

</section>
</div><!-- Main [END]-->
</div><!-- Wrapper [END]-->
<script>requireScript('script-js', '0.1.0', '/data/js/script.js', function(){cssLoaded()})</script>

</body>
</html>